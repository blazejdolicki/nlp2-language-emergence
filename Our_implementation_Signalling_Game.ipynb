{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFI90Pje3SmG"
   },
   "source": [
    "# Template for variable length Signalling Game\n",
    "\n",
    "This template is based on the [MNIST autoencoder tutorial](https://github.com/facebookresearch/EGG/blob/master/tutorials/EGG%20walkthrough%20with%20a%20MNIST%20autoencoder.ipynb) and [signal game implementation](https://github.com/facebookresearch/EGG/blob/master/egg/zoo/signal_game) provided by the [EGG library](https://github.com/facebookresearch/EGG).\n",
    "\n",
    "Some code is provided by Mathieu Bartels and Liselore Borel Rinkes at the UvA.\n",
    "\n",
    "Make sure you have a directory `SignalGame` in your Drive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:08:30.343398Z",
     "start_time": "2021-03-14T13:08:29.295685Z"
    },
    "id": "ps5ruebXSmtj"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as distance\n",
    "import scipy.stats\n",
    "import scipy\n",
    "import egg.core as core\n",
    "import egg.zoo as zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgJu9uxMLpj0"
   },
   "source": [
    "## Configuration\n",
    "Make sure to define some important configuration parameters in a convenient place, such as the number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:08:32.872966Z",
     "start_time": "2021-03-14T13:08:30.345398Z"
    },
    "id": "ngBTgCFSLpQw"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cell type of the agents: gru\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "import json\n",
    "\n",
    "# For convenience and reproducibility, we set some EGG-level command line arguments here\n",
    "opts = core.init(params=['--random_seed=7', # will initialize numpy, torch, and python RNGs\n",
    "                         '--lr=1e-3', # sets the learning rate for the selected optimizer \n",
    "                         '--batch_size=64',\n",
    "                         '--vocab_size=100',\n",
    "                         '--max_len=10',\n",
    "                         '--n_epochs=15',\n",
    "                         '--tensorboard',\n",
    "                         ]) \n",
    "\n",
    "# Other configurations that are not part of the above command line arguments we define separately. \n",
    "# Feel free to use a different format.\n",
    "_args_dict = {\n",
    "    \"architecture\" : {\n",
    "        \"embed_size\"      : 64,\n",
    "        \"hidden_sender\"   : 200,\n",
    "        \"hidden_receiver\" : 200,\n",
    "        \"cell_type\"       : 'gru',\n",
    "    },\n",
    "    \"game\" : {\n",
    "        \"num_imgs\"        : 2, # number of images the game is played with\n",
    "    },\n",
    "    \"training\" : {\n",
    "        \"temperature\"     : 1,\n",
    "        \"decay\"           : 0.9,\n",
    "        \"early_stop_accuracy\" : 0.97,\n",
    "    },\n",
    "}\n",
    "\n",
    "# A trick for having a hierarchical argument namespace from the above dict\n",
    "args = json.loads(json.dumps(_args_dict), object_hook=lambda item: types.SimpleNamespace(**item))\n",
    "\n",
    "print(\"Cell type of the agents:\", args.architecture.cell_type)\n",
    "\n",
    "# TODO: other configurations?\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRHxMztINp6W"
   },
   "source": [
    "## Vision\n",
    "In this template we use the [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) for the images and use a fixed [pre-trained vision module](https://github.com/chenyaofo/CIFAR-pretrained-models/).\n",
    "The vision module will encode the images in the dataset before the agents get to see these.\n",
    "You can also choose to make the vision module part of the agents and update the parameters during playing the game (as in the MNIST tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:08:34.964890Z",
     "start_time": "2021-03-14T13:08:32.874929Z"
    },
    "id": "ROz8NxGJKC_W"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Vision(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Vision(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=100):\n",
    "        super(Vision, self).__init__()\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = conv3x3(3, 16)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def classify(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# load pre-trained parameters\n",
    "num_classes = 100\n",
    "restnet_location = \"https://github.com/chenyaofo/CIFAR-pretrained-models/releases/download/resnet/cifar100-resnet56-2f147f26.pth\"\n",
    "vision = Vision(BasicBlock, [9, 9, 9], num_classes=num_classes).to(device)\n",
    "vision.load_state_dict(model_zoo.load_url(restnet_location))\n",
    "\n",
    "vision.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsjKgmmxKONz"
   },
   "source": [
    "## Data\n",
    "Implement a custom dataset to be used in the Signalling Game, building on top of a Dataset of your choice.\n",
    "*You could use the below code to use a featuriser for CIFAR-100 or implement your own!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:08:36.374171Z",
     "start_time": "2021-03-14T13:08:34.965893Z"
    },
    "id": "beE1Gto8bR72"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4865, 0.4409],\n",
    "        std=[0.2009, 0.1984, 0.2023]\n",
    "    ),\n",
    "])\n",
    "\n",
    "cifar_train_set = datasets.CIFAR100('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "cifar_validation_set = datasets.CIFAR100('./data', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:09:09.876047Z",
     "start_time": "2021-03-14T13:08:36.376119Z"
    },
    "id": "By_MyH0_KM33"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extract image features from train set\n",
      "Extracting features in batches: 100%|██████████| 50/50 [00:16<00:00,  3.10it/s]\n",
      "Extract image features from validation set\n",
      "Extracting features in batches: 100%|██████████| 10/10 [00:02<00:00,  3.53it/s]\n"
     ]
    }
   ],
   "source": [
    "class SignalGameDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, num_imgs, vision, classes=None, seed=1):\n",
    "        self.dataset = dataset\n",
    "        self.num_imgs = num_imgs\n",
    "        self.vision = vision\n",
    "        \n",
    "        np.random.seed(seed=seed)\n",
    "        self.img_features = self._extract_img_features() # (dataset_size, embed_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # get random images\n",
    "        random_idxs = np.random.randint(low=0, high=self.__len__(), size=self.num_imgs)\n",
    "        random_imgs = self.img_features[random_idxs]\n",
    "        # get a random permutation of integers from 0 to num_imgs-1\n",
    "        permutation = torch.randperm(self.num_imgs)\n",
    "\n",
    "        # set the target image as the first random image\n",
    "        sender_imgs = random_imgs[0].unsqueeze(dim=0)\n",
    "        \n",
    "        # permute random images for the receiver\n",
    "        receiver_imgs = random_imgs[permutation]\n",
    "        \n",
    "        # set the label\n",
    "        target = permutation.argmin()\n",
    "        \n",
    "        return sender_imgs, target, receiver_imgs\n",
    "\n",
    "    def _extract_img_features(self):\n",
    "        \"\"\"\n",
    "            We have to have to extract image features by making a forward pass through the pretrained vision model.\n",
    "            We can't do it for all images at once as there's too many of them and we would run out of memory,\n",
    "            so we do it in batches.\n",
    "        \"\"\"\n",
    "\n",
    "        # read images from dataset into a single array\n",
    "        imgs = [img for img, label in self.dataset]\n",
    "        imgs = torch.stack(imgs)\n",
    "        \n",
    "        # if you run out of memory or your laptop freezes, decrease this number and try again\n",
    "        VISION_BATCH_SIZE = 1000\n",
    "        vision_loader = torch.utils.data.DataLoader(imgs, shuffle=False, batch_size=VISION_BATCH_SIZE, num_workers=0)\n",
    "\n",
    "        # extract features from images with a vision model\n",
    "        img_features = []\n",
    "        for img in tqdm(vision_loader, desc=\"Extracting features in batches\"):\n",
    "            img = img.to(device)\n",
    "            with torch.no_grad():\n",
    "                img_features.append(self.vision(img))\n",
    "        img_features = torch.cat(img_features)\n",
    "        return img_features\n",
    "\n",
    "print(\"Extract image features from train set\")\n",
    "trainset = SignalGameDataset(cifar_train_set, args.game.num_imgs, vision)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, batch_size=opts.batch_size, num_workers=0) # was 2\n",
    "\n",
    "print(\"Extract image features from validation set\")\n",
    "validationset = SignalGameDataset(cifar_validation_set, args.game.num_imgs, vision)\n",
    "validationloader = torch.utils.data.DataLoader(validationset, shuffle=False, batch_size=opts.batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuIkT8KqK_fz"
   },
   "source": [
    "## Agent design\n",
    "We can use the Rnn Wrappers from EGG to implement a recurrent neural agent on top of your Sender and Receiver design. Here the Gumbel Softmax (GS) is used, but you can also use a [Reinforce Wrapper](https://github.com/facebookresearch/EGG/blob/master/egg/core/reinforce_wrappers.py).\n",
    "\n",
    "See [RnnSenderGS and RnnReceiverGS](https://github.com/facebookresearch/EGG/blob/master/egg/core/gs_wrappers.py) in the documentation for what happens under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6e77C67LH1U"
   },
   "source": [
    "### Sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:09:09.892042Z",
     "start_time": "2021-03-14T13:09:09.878046Z"
    },
    "id": "Av7icZpFN6Wj"
   },
   "outputs": [],
   "source": [
    "class Sender(nn.Module):\n",
    "    def __init__(self, embed_size, num_imgs, hidden_sender):\n",
    "        super(Sender, self).__init__()\n",
    "        \"\"\"\n",
    "            Note: embed_size is also the size of image features extracted from the vision model,\n",
    "            so the shape of a batch from the SignalGameDataset() is going to be (batch_size, embed_size * num_imgs)\n",
    "        \"\"\"\n",
    "        self.embed_size = embed_size\n",
    "        self.num_imgs = num_imgs\n",
    "        self.fc = nn.Linear(embed_size, hidden_sender)\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        \"\"\"\n",
    "        In our setup, the sender only sees the target image. If we wanted to give both target image and distractors to the sender,\n",
    "        we would replace `imgs.reshape(-1, self.embed_size)` with `imgs.reshape(-1, self.num_imgs*self.embed_size)`\n",
    "        \"\"\"\n",
    "        imgs = imgs.reshape(-1, self.embed_size)\n",
    "        x = self.fc(imgs).tanh()\n",
    "        return x\n",
    "\n",
    "sender = Sender(args.architecture.embed_size, args.game.num_imgs, args.architecture.hidden_sender)\n",
    "\n",
    "sender = core.RnnSenderGS(sender, opts.vocab_size, args.architecture.embed_size, args.architecture.hidden_sender, cell=args.architecture.cell_type, max_len=opts.max_len, temperature=args.training.temperature, straight_through=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oP1wc2jlLQky"
   },
   "source": [
    "### Receiver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:09:09.908053Z",
     "start_time": "2021-03-14T13:09:09.894047Z"
    },
    "id": "4ro77SNUS1rN"
   },
   "outputs": [],
   "source": [
    "class Receiver(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size):\n",
    "        super(Receiver, self).__init__()\n",
    "        self.fc = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_state, imgs):\n",
    "        # hidden_state is the message passed by the sender and is of shape (batch_size, hidden_size)\n",
    "        # imgs is of shape (batch_size, num_imgs, embed_size)\n",
    "        embed_imgs = self.fc(imgs).tanh() # (batch_size, num_imgs, hidden_size)\n",
    "        \n",
    "        hidden_state = torch.unsqueeze(hidden_state, dim=-1) # (batch_size, hidden_size, 1)\n",
    "\n",
    "        # (batch_size, num_imgs, hidden_size) x (batch_size, hidden_size, 1) = (batch_size, num_imgs, 1)\n",
    "        # because (num_imgs, hidden_size) x (hidden_size, 1) = (num_imgs, 1)\n",
    "        energies = torch.matmul(embed_imgs, hidden_state) \n",
    "\n",
    "        return energies.squeeze() # (batch_size, num_imgs)\n",
    "\n",
    "receiver = Receiver(args.architecture.hidden_receiver, args.architecture.embed_size)\n",
    "\n",
    "receiver = core.RnnReceiverGS(receiver, opts.vocab_size, args.architecture.embed_size,\n",
    "                    args.architecture.hidden_receiver, cell=args.architecture.cell_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1gSNpSoLUe5"
   },
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:09:09.923125Z",
     "start_time": "2021-03-14T13:09:09.911060Z"
    },
    "id": "OOr9uayQLSdj"
   },
   "outputs": [],
   "source": [
    "def loss(_sender_input,  _message, _receiver_input, receiver_output, _labels):\n",
    "    acc = (receiver_output.argmax(dim=1) == _labels).detach().float()\n",
    "    loss = F.cross_entropy(receiver_output, _labels, reduction=\"none\")\n",
    "    # print('Loss: ', loss.mean().cpu().item(), 'Acc: ', acc.mean().cpu().item())\n",
    "    return loss, {'acc': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnCBbeyoLYPw"
   },
   "source": [
    "## Game setup and training\n",
    "Use `core.SenderReceiverRnnGS` for creating a variable message length game, set the optimizer and other options, and start training the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:09:09.939132Z",
     "start_time": "2021-03-14T13:09:09.925042Z"
    },
    "id": "PCVqdUY1TCKX"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:09:09.955130Z",
     "start_time": "2021-03-14T13:09:09.940045Z"
    },
    "id": "U8og00m4e7tF"
   },
   "outputs": [],
   "source": [
    "model_prefix = f\"maxlen_{opts.max_len}\" # Example\n",
    "models_path = \"/content/drive/My Drive/SignalGame/models\" # location where we store trained models\n",
    "\n",
    "checkpointer = core.callbacks.CheckpointSaver(checkpoint_path=models_path, checkpoint_freq=0, prefix=model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:09:10.176041Z",
     "start_time": "2021-03-14T13:09:09.956046Z"
    },
    "id": "1xdWxbaBfMC5"
   },
   "outputs": [],
   "source": [
    "game = core.SenderReceiverRnnGS(sender, receiver, loss)\n",
    "optimizer = torch.optim.Adam(game.parameters())\n",
    "\n",
    "callbacks = [core.TemperatureUpdater(agent=game.sender, decay=args.training.decay, minimum=0.1),\n",
    "             core.ConsoleLogger(as_json=True, print_train_loss=True),\n",
    "             core.TensorboardLogger(),\n",
    "             core.EarlyStopperAccuracy(args.training.early_stop_accuracy),\n",
    "             checkpointer]\n",
    "\n",
    "trainer = core.Trainer(\n",
    "    game=game, optimizer=optimizer, train_data=trainloader,\n",
    "    validation_data=validationloader, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:15:01.827787Z",
     "start_time": "2021-03-14T13:09:10.177042Z"
    },
    "id": "MOJw2EBvfRk9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\"loss\": 0.29803892970085144, \"acc\": 0.8338599801063538, \"length\": 7.530139923095703, \"mode\": \"train\", \"epoch\": 1}\n",
      "{\"loss\": 0.16294455528259277, \"acc\": 0.9171000123023987, \"length\": 8.18690013885498, \"mode\": \"test\", \"epoch\": 1}\n",
      "{\"loss\": 0.1451311558485031, \"acc\": 0.9274399876594543, \"length\": 8.542619705200195, \"mode\": \"train\", \"epoch\": 2}\n",
      "{\"loss\": 0.10232457518577576, \"acc\": 0.9528999924659729, \"length\": 8.155099868774414, \"mode\": \"test\", \"epoch\": 2}\n",
      "{\"loss\": 0.09853953868150711, \"acc\": 0.9538999795913696, \"length\": 7.04856014251709, \"mode\": \"train\", \"epoch\": 3}\n",
      "{\"loss\": 0.08425280451774597, \"acc\": 0.9613999724388123, \"length\": 6.052299976348877, \"mode\": \"test\", \"epoch\": 3}\n",
      "{\"loss\": 0.06785829365253448, \"acc\": 0.9699400067329407, \"length\": 7.7115797996521, \"mode\": \"train\", \"epoch\": 4}\n",
      "{\"loss\": 0.05962985381484032, \"acc\": 0.9725000262260437, \"length\": 7.402100086212158, \"mode\": \"test\", \"epoch\": 4}\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs=opts.n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I checked and setting the random seed works correctly - the results are exactly the same for two attempts with the same seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:15:01.875793Z",
     "start_time": "2021-03-14T13:15:01.845785Z"
    },
    "id": "i4rEbi4WThKw"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ERROR: Could not find `tensorboard`. Please ensure that your PATH\ncontains an executable `tensorboard` program, or explicitly specify\nthe path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\nenvironment variable."
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_nA9yyIMt9r"
   },
   "source": [
    "## Evaluation of the emergent languages\n",
    "Next up, start analysing the languages and other behaviour you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:21:46.316521Z",
     "start_time": "2021-03-14T13:21:27.107943Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[25 11 11 25 11 11 11  0]\n",
      "[68 68 68 77 68 68 77 68 68 25  0]\n",
      "[68 68 68 77 68 68 68 77 82 77  0]\n",
      "[77 77 82 77 77 82 77 77 82 77  0]\n",
      "[77 82 82 82 77 82 82 82 82 82  0]\n",
      "[25 25  0]\n",
      "[82 82 82 82 82 82 82 82 82 82  0]\n",
      "[25 25 25 25 25 25 25 11 25 11  0]\n",
      "[72 68 25 62 11 11 11 11 11  0]\n",
      "[68 82 77 82 77 82 77 82 77 82  0]\n",
      "[72 62 82  0]\n",
      "[11  0]\n",
      "[62  0]\n",
      "[85 11  0]\n",
      "[77 68 25 82 25 11 11 11 11 11  0]\n",
      "[11 11  0]\n",
      "[72  0]\n",
      "[68 77 82 77 68 68 77 82 11 68  0]\n",
      "[85 11 62 11 11 82 11 82  0]\n",
      "[72  0]\n",
      "[11 82 77 82 11 82 82 82 11 82  0]\n",
      "[82 82 82 82 82 82 82 82 82 82  0]\n",
      "[82 82 82 82 11 82 82 82 82 82  0]\n",
      "[68 77 82 77 82 11 82 77 82 82  0]\n",
      "[25 25 62 62 62 11  0]\n",
      "[82 11 11 11 11 11 11 11 82 11  0]\n",
      "[77 82 77 82 82 82 82 82 82 82  0]\n",
      "[72 62  0]\n",
      "[82 82 77 82 82 77 82 82 77 82  0]\n",
      "[72  0]\n",
      "[72 77 82 77 82 82 77 82 82 82  0]\n",
      "[72  0]\n",
      "[72 62 82 62 82 82 82 82  0]\n",
      "[82 77 82 77 82 82 77 82 82 77  0]\n",
      "[72 72 82 72 82 82 72 82 82 82  0]\n",
      "[72  2 72 82 82 62 62 82 82 82  0]\n",
      "[72  0]\n",
      "[68 25 11 11 11 11 11 25 11 11  0]\n",
      "[68 68 68 68 77 82 68 77 82 68  0]\n",
      "[68 68 77 68 68 77 68 68 25 82  0]\n",
      "[85 62 62 11 11 11 11 82  0]\n",
      "[11 11 11 82  0]\n",
      "[11 11 82 11 11 82 11 82 11 82  0]\n",
      "[72 77 82 72 82 82 77 82 82 82  0]\n",
      "[82 82 82  0]\n",
      "[82 82 82 82 82 82 82  0]\n",
      "[11 82 82 82 82 82 82  0]\n",
      "[11 82 11 11 82 11 82 11 82 82  0]\n",
      "[82 68 82 68 82 68 68 82 77 82  0]\n",
      "[11 11 82 82 82  0]\n",
      "[82 82 11 82 82 11 82 11 82 82  0]\n",
      "[25 25 62 11  0]\n",
      "[72  0]\n",
      "[68 82 77 82 77 82 82 11 82 82  0]\n",
      "[72 62  0]\n",
      "[82 77 82 82 82 82 82 82 82 82  0]\n",
      "[82 11 11 11 11 11 82 11 11 82  0]\n",
      "[82 82 11 82 11 82 11 82 11 82  0]\n",
      "[25 25 25 11 25 11 25 11  0]\n",
      "[11 11 62 62 62 11 11  0]\n",
      "[85 62 11 11  0]\n",
      "[72  0]\n",
      "[11 11 11  0]\n",
      "[25 62  0]\n",
      "[25 25 11 62 62 11 11  0]\n",
      "[77 82 77 82 77 82 77 82 77 82  0]\n",
      "[77 77 77 82 77 77 82 77 77 82  0]\n",
      "[72  0]\n",
      "[25 25 62  0]\n",
      "[25 62 62  0]\n",
      "[82 82 82 82 82 82  0]\n",
      "[25 25 25  0]\n",
      "[25 25  0]\n",
      "[82 82 82 82 82 82 82 82 82 82  0]\n",
      "[11  0]\n",
      "[82 82 82 82 82 82  0]\n",
      "[11 11 11 11  0]\n",
      "[72 62 11 82  0]\n",
      "[72 25 62  0]\n",
      "[82 68 68 68 68 68 68 77 82 68  0]\n",
      "[25 62 62  0]\n",
      "[72 72  2 72 82 82 82  0]\n",
      "[72  0]\n",
      "[62 62 62 62 11  0]\n",
      "[82 68 82 11 68 82 11 11 11 82  0]\n",
      "[72  0]\n",
      "[72  0]\n",
      "[25 25 62 11  0]\n",
      "[72  0]\n",
      "[72  0]\n",
      "[25 25 11 25 62 11  0]\n",
      "[25 25 11  0]\n",
      "[72  0]\n",
      "[72 72 82 62 82 82  0]\n",
      "[77 68 77 68 77 68 77 68 68 77  0]\n",
      "[72  0]\n",
      "[11 62 62 62 11  0]\n",
      "[82 82 82 82  0]\n",
      "[77 77 82 77 82 82 77 82 82 82  0]\n",
      "[82 77 82 82 77 82 82 77 82 82  0]\n",
      "[72  0]\n",
      "[82 11 11 82 11 82 82 82 11 82  0]\n"
     ]
    }
   ],
   "source": [
    "from egg.zoo.objects_game.util import dump_sender_receiver\n",
    "# Check out https://github.com/facebookresearch/EGG/blob/aba2489e78f0b6e202bf2c6138fde41bf9056cb5/egg/zoo/objects_game/util.py\n",
    "\n",
    "sender_inputs, messages, receiver_inputs, receiver_outputs, labels = dump_sender_receiver(game, validationloader, True, variable_length=True, device=device)\n",
    "messages = [message.cpu().numpy() for message in messages]\n",
    "for i, m in enumerate(messages):\n",
    "    print(str(m))\n",
    "    if i>100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian noise vector test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b34bd38b38>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.919844pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.919844\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-03-18T22:34:36.392346</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.919844 \r\nL 251.565 248.919844 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 225.041719 \r\nL 244.365 225.041719 \r\nL 244.365 7.601719 \r\nL 26.925 7.601719 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p61c2298898)\">\r\n    <image height=\"218\" id=\"image3a0221feb5\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-7.041719\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m775b2499c8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.3225\" xlink:href=\"#m775b2499c8\" y=\"225.041719\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.14125 239.640156)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"64.2975\" xlink:href=\"#m775b2499c8\" y=\"225.041719\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(61.11625 239.640156)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.2725\" xlink:href=\"#m775b2499c8\" y=\"225.041719\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(91.91 239.640156)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"132.2475\" xlink:href=\"#m775b2499c8\" y=\"225.041719\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(125.885 239.640156)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.2225\" xlink:href=\"#m775b2499c8\" y=\"225.041719\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(159.86 239.640156)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"200.1975\" xlink:href=\"#m775b2499c8\" y=\"225.041719\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(193.835 239.640156)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.1725\" xlink:href=\"#m775b2499c8\" y=\"225.041719\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(227.81 239.640156)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m89152ccce4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m89152ccce4\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m89152ccce4\" y=\"44.974219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 48.773437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m89152ccce4\" y=\"78.949219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m89152ccce4\" y=\"112.924219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m89152ccce4\" y=\"146.899219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m89152ccce4\" y=\"180.874219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m89152ccce4\" y=\"214.849219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 225.041719 \r\nL 26.925 7.601719 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 225.041719 \r\nL 244.365 7.601719 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 225.041719 \r\nL 244.365 225.041719 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.601719 \r\nL 244.365 7.601719 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p61c2298898\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.601719\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def init_dataset(num_imgs):\n",
    "    samples = []\n",
    "    for i in range(num_imgs):\n",
    "        noise_vec = np.random.normal(size=(3, 32, 32))\n",
    "        # What label to give to these noise vectors?\n",
    "        samples.append((torch.from_numpy(noise_vec).float(), 0))\n",
    "    return samples\n",
    "\n",
    "test = init_dataset(1000)\n",
    "\n",
    "arr = test[0][0]\n",
    "\n",
    "plt.imshow(arr.numpy()[0], cmap='GnBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extract image features from test set\n",
      "Extracting features in batches: 100%|██████████| 10/10 [00:02<00:00,  3.47it/s]\n"
     ]
    }
   ],
   "source": [
    "class GaussianNoiseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_imgs, vision, dataset_size, classes=None, seed=1, im_size=(3, 32, 32), mean=0.0, std_dev=1.0):\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_dev\n",
    "        self.num_imgs = num_imgs\n",
    "        self.im_size = im_size\n",
    "        self.dataset_size = dataset_size\n",
    "        self.dataset = self._init_dataset()\n",
    "        \n",
    "        self.vision = vision\n",
    "        \n",
    "        np.random.seed(seed=seed)\n",
    "        self.img_features = self._extract_img_features() # (dataset_size, embed_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # get random images\n",
    "        random_idxs = np.random.randint(low=0, high=self.__len__(), size=self.num_imgs)\n",
    "        random_imgs = self.img_features[random_idxs]\n",
    "        # get a random permutation of integers from 0 to num_imgs-1\n",
    "        permutation = torch.randperm(self.num_imgs)\n",
    "\n",
    "        # set the target image as the first random image\n",
    "        sender_imgs = random_imgs[0].unsqueeze(dim=0)\n",
    "        \n",
    "        # permute random images for the receiver\n",
    "        receiver_imgs = random_imgs[permutation]\n",
    "        \n",
    "        # set the label\n",
    "        target = permutation.argmin()\n",
    "        \n",
    "        return sender_imgs, target, receiver_imgs\n",
    "    \n",
    "    def _init_dataset(self):\n",
    "        samples = []\n",
    "        for i in range(self.dataset_size):\n",
    "            noise_vec = np.random.normal(loc=self.mean, scale=self.std_dev, size=self.im_size)\n",
    "            # What label to give to these noise vectors?\n",
    "            samples.append((torch.from_numpy(noise_vec).float(), 0))\n",
    "        return samples\n",
    "\n",
    "    def _extract_img_features(self):\n",
    "        \"\"\"\n",
    "            We have to extract image features by making a forward pass through the pretrained vision model.\n",
    "            We can't do it for all images at once as there's too many of them and we would run out of memory,\n",
    "            so we do it in batches.\n",
    "        \"\"\"\n",
    "\n",
    "        # read images from dataset into a single array\n",
    "        imgs = [img for img, label in self.dataset]\n",
    "        imgs = torch.stack(imgs)\n",
    "        \n",
    "        # if you run out of memory or your laptop freezes, decrease this number and try again\n",
    "        VISION_BATCH_SIZE = 1000\n",
    "        vision_loader = torch.utils.data.DataLoader(imgs, shuffle=False, batch_size=VISION_BATCH_SIZE, num_workers=0)\n",
    "\n",
    "        # extract features from images with a vision model\n",
    "        img_features = []\n",
    "        for img in tqdm(vision_loader, desc=\"Extracting features in batches\"):\n",
    "            img = img.to(device)\n",
    "            with torch.no_grad():\n",
    "                img_features.append(self.vision(img))\n",
    "        img_features = torch.cat(img_features)\n",
    "        return img_features\n",
    "\n",
    "print(\"Extract image features from test set\")\n",
    "testset_noise = GaussianNoiseDataset(num_imgs=args.game.num_imgs, vision=vision, dataset_size=10000)\n",
    "testloader_noise = torch.utils.data.DataLoader(testset_noise, shuffle=False, batch_size=opts.batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\"loss\": 0.06062287464737892, \"acc\": 0.9728999733924866, \"length\": 8.660679817199707, \"mode\": \"train\", \"epoch\": 1}\n",
      "{\"loss\": 0.6836126446723938, \"acc\": 0.5695000290870667, \"length\": 5.9934000968933105, \"mode\": \"test\", \"epoch\": 1}\n",
      "{\"loss\": 0.05801570415496826, \"acc\": 0.9743800163269043, \"length\": 8.02810001373291, \"mode\": \"train\", \"epoch\": 2}\n",
      "{\"loss\": 0.6931045651435852, \"acc\": 0.5879999995231628, \"length\": 6.507199764251709, \"mode\": \"test\", \"epoch\": 2}\n",
      "{\"loss\": 0.05595071241259575, \"acc\": 0.97434002161026, \"length\": 8.17039966583252, \"mode\": \"train\", \"epoch\": 3}\n",
      "{\"loss\": 0.6989288330078125, \"acc\": 0.550000011920929, \"length\": 8.038000106811523, \"mode\": \"test\", \"epoch\": 3}\n",
      "{\"loss\": 0.06950756162405014, \"acc\": 0.968500018119812, \"length\": 8.47107982635498, \"mode\": \"train\", \"epoch\": 4}\n",
      "{\"loss\": 0.7391089200973511, \"acc\": 0.5649999976158142, \"length\": 9.411199569702148, \"mode\": \"test\", \"epoch\": 4}\n",
      "{\"loss\": 0.06804963201284409, \"acc\": 0.9696999788284302, \"length\": 8.951319694519043, \"mode\": \"train\", \"epoch\": 5}\n",
      "{\"loss\": 0.7334177494049072, \"acc\": 0.5256999731063843, \"length\": 6.616000175476074, \"mode\": \"test\", \"epoch\": 5}\n",
      "{\"loss\": 0.055542558431625366, \"acc\": 0.9752200245857239, \"length\": 9.254940032958984, \"mode\": \"train\", \"epoch\": 6}\n",
      "{\"loss\": 0.9488303065299988, \"acc\": 0.5507000088691711, \"length\": 7.966899871826172, \"mode\": \"test\", \"epoch\": 6}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cae6e8adeb75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     validation_data=testloader_noise, callbacks=callbacks)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Blazej\\Programs\\Anaconda\\envs\\le-nlp2\\lib\\site-packages\\egg\\core\\trainers.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_epochs)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa: E226\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_interaction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Blazej\\Programs\\Anaconda\\envs\\le-nlp2\\lib\\site-packages\\egg\\core\\trainers.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    225\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Blazej\\Programs\\Anaconda\\envs\\le-nlp2\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Blazej\\Programs\\Anaconda\\envs\\le-nlp2\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Blazej\\Programs\\Anaconda\\envs\\le-nlp2\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                    group['eps'])\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Blazej\\Programs\\Anaconda\\envs\\le-nlp2\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = core.Trainer(\n",
    "    game=game, optimizer=optimizer, train_data=trainloader,\n",
    "    validation_data=testloader_noise, callbacks=callbacks)\n",
    "\n",
    "trainer.train(n_epochs=opts.n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(test_dataloader=testloader_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SignalGameDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, dataset, num_imgs, vision, classes=None, seed=1):\n",
    "#         self.dataset = dataset\n",
    "#         self.num_imgs = num_imgs\n",
    "#         self.vision = vision\n",
    "        \n",
    "#         np.random.seed(seed=seed)\n",
    "#         self.img_features = self._extract_img_features() # (dataset_size, embed_size)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         # get random images\n",
    "#         random_idxs = np.random.randint(low=0, high=self.__len__(), size=self.num_imgs)\n",
    "#         random_imgs = self.img_features[random_idxs]\n",
    "#         # get a random permutation of integers from 0 to num_imgs-1\n",
    "#         permutation = torch.randperm(self.num_imgs)\n",
    "\n",
    "#         # set the target image as the first random image\n",
    "#         sender_imgs = random_imgs[0].unsqueeze(dim=0)\n",
    "        \n",
    "#         # permute random images for the receiver\n",
    "#         receiver_imgs = random_imgs[permutation]\n",
    "        \n",
    "#         # set the label\n",
    "#         target = permutation.argmin()\n",
    "        \n",
    "#         return sender_imgs, target, receiver_imgs\n",
    "\n",
    "#     def _extract_img_features(self):\n",
    "#         \"\"\"\n",
    "#             We have to have to extract image features by making a forward pass through the pretrained vision model.\n",
    "#             We can't do it for all images at once as there's too many of them and we would run out of memory,\n",
    "#             so we do it in batches.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # read images from dataset into a single array\n",
    "#         imgs = [img for img, label in self.dataset]\n",
    "#         imgs = torch.stack(imgs)\n",
    "        \n",
    "#         # if you run out of memory or your laptop freezes, decrease this number and try again\n",
    "#         VISION_BATCH_SIZE = 1000\n",
    "#         vision_loader = torch.utils.data.DataLoader(imgs, shuffle=False, batch_size=VISION_BATCH_SIZE, num_workers=0)\n",
    "\n",
    "#         # extract features from images with a vision model\n",
    "#         img_features = []\n",
    "#         for img in tqdm(vision_loader, desc=\"Extracting features in batches\"):\n",
    "#             img = img.to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 img_features.append(self.vision(img))\n",
    "#         img_features = torch.cat(img_features)\n",
    "#         return img_features\n",
    "\n",
    "\n",
    "# class GaussianNoiseDataset(Dataset):\n",
    "#     def __init__(self, dataset_size, game_size, vision, embedding_size, dataset_name, device, \n",
    "#                  sender_has_distractor=True, classes=None, im_size=(32,32), mean=0.0, std_dev=1.0):\n",
    "#         self.mean = mean\n",
    "#         self.std_dev = std_dev\n",
    "#         self.dataset_name = dataset_name\n",
    "#         self.dataset_size = dataset_size\n",
    "#         self.im_size = im_size\n",
    "#         self.dataset = self._init_dataset()\n",
    "        \n",
    "#         self.game_size = game_size\n",
    "#         self.vision = vision\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.embeddings = self.pre_process_image_embeddings(16)\n",
    "#         self.sender_has_distractor = sender_has_distractor\n",
    "#         self.device = device\n",
    "\n",
    "#     def _init_dataset(self):\n",
    "#         samples = []\n",
    "#         for i in range(self.dataset_size):\n",
    "#             noise_vec = np.random.normal(loc=self.mean, scale=self.std_dev, size=self.im_size)\n",
    "#             samples.append(noise_vec)\n",
    "#         return samples\n",
    "            \n",
    "#     def pre_process_image_embeddings(self, batch_size):\n",
    "#         if os.path.isfile(f\"image_embeddings_{self.dataset_name}.pkl\"):\n",
    "#             return torch.load( open(f\"image_embeddings_{self.dataset_name}.pkl\", \"rb\" ) )\n",
    "#         trainloader = torch.utils.data.DataLoader(self.dataset, shuffle=False,\n",
    "#                                           batch_size=batch_size, num_workers=2)\n",
    "        \n",
    "#         image_embeddings = torch.zeros((len(self.dataset), self.embedding_size))\n",
    "#         labels = torch.zeros(len(self.dataset))\n",
    "#         for i, (x, y) in enumerate(tqdm(trainloader)):\n",
    "#           x = x.to(device)\n",
    "#           with torch.no_grad():\n",
    "#             embedding = self.vision(x).cpu()\n",
    "#           image_embeddings[i*batch_size:(i+1) * batch_size, :] = embedding\n",
    "#           labels[i*batch_size:(i+1) * batch_size]  = y\n",
    "        \n",
    "#         torch.save(image_embeddings, open(f\"image_embeddings_{self.dataset_name}.pkl\", \"wb\" ))\n",
    "#         return image_embeddings\n",
    "\n",
    "#     def get_item_info(self, index):\n",
    "#         image, classlabel = self.dataset[index]\n",
    "#         return image, classlabel\n",
    "    \n",
    "#     def __len__(self):\n",
    "#       return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         dataset = self.embeddings\n",
    "#         game_size = self.game_size\n",
    "#         target_image = dataset[item]\n",
    "\n",
    "#         indices = get_random_indices(item, range(self.__len__()), game_size-1)\n",
    "#         images = [target_image] + [dataset[indice] for indice in indices]\n",
    "\n",
    "#         sender_images = torch.stack(images, dim=0)\n",
    "\n",
    "#         perm = torch.randperm(game_size)\n",
    "#         receiver_imgs = sender_images[perm]\n",
    "#         target = torch.argmin(perm)\n",
    "        \n",
    "#         if not self.sender_has_distractor:\n",
    "#             sender_images = target_image\n",
    "\n",
    "#         return sender_images, target, receiver_imgs\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     testset_noise = GaussianNoiseDataset(dataset_size=1000, game_size=2, vision=vision, embedding_size=64, \n",
    "#                 dataset_name=\"gaussian_noise\", device=device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Template_Signalling_Game--CIFAR-100.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}